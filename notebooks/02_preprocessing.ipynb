{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1baa2d14",
   "metadata": {},
   "source": [
    "# Imports "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8dd18836",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "711383d6",
   "metadata": {},
   "source": [
    "# Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5df5c810",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(r\"D:\\Python\\SmartSignalAI\\data\\An Urban Multi-Operator QoE-Aware Dataset for Cell\\raw_dataset.csv\")\n",
    "print(\"Dataset loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce53581b",
   "metadata": {},
   "source": [
    "# Feature Engineering - Distance Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f0d03095",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the geographical distance between:\n",
    "# - User device location\n",
    "# - Cell tower (Node) location\n",
    "# Using the Haversine formula\n",
    "\n",
    "def haversine(lat1, lon1, lat2, lon2):\n",
    "    R = 6371  # Earth radius in kilometers\n",
    "    \n",
    "    lat1 = np.radians(lat1)\n",
    "    lon1 = np.radians(lon1)\n",
    "    lat2 = np.radians(lat2)\n",
    "    lon2 = np.radians(lon2)\n",
    "\n",
    "    dlat = lat2 - lat1\n",
    "    dlon = lon2 - lon1\n",
    "\n",
    "    a = np.sin(dlat/2)**2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon/2)**2\n",
    "    c = 2 * np.arcsin(np.sqrt(a))\n",
    "\n",
    "    return R * c"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a16be9bd",
   "metadata": {},
   "source": [
    "# Create Distance Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f60704df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distance feature created successfully!\n"
     ]
    }
   ],
   "source": [
    "df['Distance_km'] = haversine(\n",
    "    df['Latitude'],\n",
    "    df['Longitude'],\n",
    "    df['Node_Latitude'],\n",
    "    df['Node_Longitude']\n",
    ")\n",
    "\n",
    "print(\"Distance feature created successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b05e6f6",
   "metadata": {},
   "source": [
    "# Feature & Target Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "100e117a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature matrix shape: (30925, 6)\n",
      "Target shape: (30925,)\n"
     ]
    }
   ],
   "source": [
    "# Selected features for RSRP prediction:\n",
    "# - Distance_km\n",
    "# - Speed\n",
    "\n",
    "# Target:\n",
    "# - Level (RSRP)\n",
    "\n",
    "# Continuous features\n",
    "cont_features = ['Distance_km', 'Speed', 'ElapsedTime']\n",
    "\n",
    "# Categorical features (need encoding)\n",
    "cat_features = ['Operatorname', 'NetworkTech', 'Mobility']\n",
    "\n",
    "# Target\n",
    "y = df['Level']  # RSRP\n",
    "\n",
    "# Combine continuous + categorical\n",
    "X = df[cont_features + cat_features]\n",
    "\n",
    "print(\"Feature matrix shape:\", X.shape)\n",
    "print(\"Target shape:\", y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "739c9e81",
   "metadata": {},
   "source": [
    "# Train / Validation / Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9ae0bc6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (21647, 6)\n",
      "Validation shape: (4639, 6)\n",
      "Test shape: (4639, 6)\n"
     ]
    }
   ],
   "source": [
    "# Train-Validation-Test Split\n",
    "\n",
    "# First split: Train + Temp (70% train, 30% temp)\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "    X, y,\n",
    "    test_size=0.30,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Second split: Validation + Test (50% validation, 50% test of the temp set)\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_temp, y_temp,\n",
    "    test_size=0.50,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(\"Train shape:\", X_train.shape)\n",
    "print(\"Validation shape:\", X_val.shape)\n",
    "print(\"Test shape:\", X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dad54e30",
   "metadata": {},
   "source": [
    "# Outlier Handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "10d5d467",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Cleaning: Remove outliers based on RSRP values\n",
    "# Outlier handling (y_train clipping -50 to -140)\n",
    "# Filtering out only â€œbadâ€ target rows\n",
    "# Features corresponding to those rows are removed automatically\n",
    "\n",
    "mask = (y_train >= -140) & (y_train <= -50)\n",
    "\n",
    "X_train = X_train[mask]\n",
    "y_train = y_train[mask]\n",
    "\n",
    "# Validation = simulate unseen data\n",
    "\n",
    "# Test = simulate real-world deployment\n",
    "\n",
    "# Real world WILL contain noise\n",
    "\n",
    "# If you clean val/test, you are artificially making your model look better."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "272f2a6e",
   "metadata": {},
   "source": [
    "# Feauture Scaling (NO leakage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7b4409eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing (scaling + encoding) completed successfully!\n"
     ]
    }
   ],
   "source": [
    "# Feature Scaling - Standardization\n",
    "# Scaling & Encoding in One Step with ColumnTransformer\n",
    "# Feature Scaling: Standardize features to have mean=0 and std=1\n",
    "\n",
    "# ColumnTransformer to apply StandardScaler to continuous features and OneHotEncoder to categorical features in one step\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), cont_features),\n",
    "        ('cat', OneHotEncoder(sparse_output=False, handle_unknown='ignore'), cat_features)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Fit on training data only\n",
    "X_train_processed = preprocessor.fit_transform(X_train)\n",
    "\n",
    "# Transform validation and test\n",
    "X_val_processed   = preprocessor.transform(X_val)\n",
    "X_test_processed  = preprocessor.transform(X_test)\n",
    "\n",
    "print(\"Preprocessing (scaling + encoding) completed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aae44e9",
   "metadata": {},
   "source": [
    "# Saving Processed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cad8581f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All processed datasets saved successfully!\n"
     ]
    }
   ],
   "source": [
    "# Save preprocessed data and scaler for future use\n",
    "\n",
    "joblib.dump(X_train_processed, r\"D:\\Python\\SmartSignalAI\\data\\X_train.pkl\") # Preprocessed train features & target\n",
    "joblib.dump(y_train, r\"D:\\Python\\SmartSignalAI\\data\\y_train.pkl\")\n",
    "\n",
    "joblib.dump(X_val_processed, r\"D:\\Python\\SmartSignalAI\\data\\X_val.pkl\")    # Preprocessed validation data\n",
    "joblib.dump(y_val, r\"D:\\Python\\SmartSignalAI\\data\\y_val.pkl\")\n",
    "\n",
    "joblib.dump(X_test_processed, r\"D:\\Python\\SmartSignalAI\\data\\X_test.pkl\")  # Preprocessed test data\n",
    "joblib.dump(y_test, r\"D:\\Python\\SmartSignalAI\\data\\y_test.pkl\")\n",
    "\n",
    "joblib.dump(preprocessor, r\"D:\\Python\\SmartSignalAI\\data\\preprocessor.pkl\")\n",
    "\n",
    "print(\"All processed datasets saved successfully!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
